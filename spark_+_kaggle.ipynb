{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXR+ToxeOfs3fnkIsz5iU3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danushika06/Dpt/blob/main/spark_%2B_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSxpo_ahNl5d",
        "outputId": "4f1eb84d-f396-4b5a-ebcc-f71a53c750a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1\n",
        "!pip install -q pyspark openml\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Replaces Kaggle download with OpenML mirror (same schema) ---\n",
        "import openml, pandas as pd, os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean, when, count, isnan\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# Download dataset from OpenML and save as creditcard.csv (schema-compatible)\n",
        "d = openml.datasets.get_dataset(1597)  # \"Credit Card Fraud\" (ULB)\n",
        "X, y, _, _ = d.get_data(target=d.default_target_attribute)\n",
        "ccf = pd.concat([X, y.rename(\"Class\")], axis=1)\n",
        "\n",
        "path = \"/content/openml_ccf\"          # keep a folder \"path\" like KaggleHub returned\n",
        "os.makedirs(path, exist_ok=True)\n",
        "file_path = f\"{path}/creditcard.csv\"  # same filename used downstream\n",
        "ccf.to_csv(file_path, index=False)\n",
        "print(\"üìÅ Path to dataset files:\", path)\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataPreprocessingChallenge\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Spark session created successfully!\")\n",
        "\n",
        "# Read CSV file\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "print(\"‚úÖ Dataset loaded successfully.\")\n",
        "data.printSchema()\n",
        "data.show(5)\n",
        "\n",
        "# Count missing values\n",
        "missing_counts = data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data.columns])\n",
        "print(\"üîç Missing value count per column:\")\n",
        "missing_counts.show()\n",
        "\n",
        "# Fill missing numeric columns with mean\n",
        "numeric_cols = [c for c, t in data.dtypes if t in ['double', 'int']]\n",
        "for column in numeric_cols:\n",
        "    mean_value = data.select(mean(col(column))).collect()[0][0]\n",
        "    data = data.fillna({column: mean_value})\n",
        "\n",
        "# Fill missing categorical columns (if any)\n",
        "categorical_cols = [c for c, t in data.dtypes if t == 'string']\n",
        "for column in categorical_cols:\n",
        "    data = data.fillna({column: 'Unknown'})\n",
        "\n",
        "print(\"‚úÖ Missing values handled.\")\n",
        "\n",
        "for column in numeric_cols:\n",
        "    data = data.withColumn(column, col(column).cast(\"double\"))\n",
        "\n",
        "data.printSchema()\n",
        "print(\"‚úÖ Data types standardized.\")\n",
        "\n",
        "before = data.count()\n",
        "data = data.dropDuplicates()\n",
        "after = data.count()\n",
        "\n",
        "print(f\"üßπ Removed {before - after} duplicate rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c386001"
      },
      "source": [
        "# After loading the CSV into the Spark DataFrame 'data'\n",
        "output_parquet_path = \"/content/openml_ccf_parquet\"\n",
        "data.write.parquet(output_parquet_path, mode=\"overwrite\")\n",
        "print(f\"‚úÖ Data saved as Parquet at: {output_parquet_path}\")\n",
        "\n",
        "# To load the data faster in subsequent runs, you can use:\n",
        "# data = spark.read.parquet(output_parquet_path)\n",
        "# print(\"‚úÖ Data loaded from Parquet.\")\n",
        "# data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Download and Preprocess Credit Card Fraud Data\n",
        "# =========================================\n",
        "\n",
        "# 1Ô∏è‚É£ Import libraries\n",
        "import os\n",
        "import openml\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Download dataset (OpenML, same schema)\n",
        "# -------------------------------\n",
        "d = openml.datasets.get_dataset(1597)  # \"Credit Card Fraud\"\n",
        "X, y, _, _ = d.get_data(target=d.default_target_attribute)\n",
        "df_ = pd.concat([X, y.rename(\"Class\")], axis=1)\n",
        "\n",
        "data_path = \"/content/openml_ccf_2\"\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "csv_file = os.path.join(data_path, \"creditcard.csv\")\n",
        "df_.to_csv(csv_file, index=False)\n",
        "print(\"Dataset downloaded to:\", data_path)\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Initialize Spark\n",
        "# -------------------------------\n",
        "spark = SparkSession.builder.appName(\"CreditCardFraud\").getOrCreate()\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Load CSV into Spark DataFrame\n",
        "# -------------------------------\n",
        "data = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
        "print(\"=== Raw Data Sample ===\")\n",
        "data.show(5)\n",
        "\n",
        "# -------------------------------\n",
        "# 5Ô∏è‚É£ Handle Missing Values\n",
        "# -------------------------------\n",
        "numeric_cols = [c.name for c in data.schema.fields if str(c.dataType) in ['IntegerType', 'DoubleType']]\n",
        "data = data.fillna({c: 0 for c in numeric_cols})\n",
        "\n",
        "# -------------------------------\n",
        "# 6Ô∏è‚É£ Remove Duplicates\n",
        "# -------------------------------\n",
        "data = data.dropDuplicates()\n",
        "\n",
        "# -------------------------------\n",
        "# 7Ô∏è‚É£ Feature Engineering\n",
        "# -------------------------------\n",
        "# Create 'TransactionHour' from 'Time' (seconds)\n",
        "data = data.withColumn(\"TransactionHour\", ((col(\"Time\") / 3600) % 24).cast(\"int\"))\n",
        "\n",
        "# -------------------------------\n",
        "# 8Ô∏è‚É£ Assemble Features\n",
        "# -------------------------------\n",
        "feature_cols = [c for c in data.columns if c not in ['Time', 'Class']]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "assembled = assembler.transform(data)\n",
        "\n",
        "# -------------------------------\n",
        "# 9Ô∏è‚É£ Standardize Features\n",
        "# -------------------------------\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "scaler_model = scaler.fit(assembled)\n",
        "scaled_data = scaler_model.transform(assembled)\n",
        "\n",
        "scaled_data.select(\"scaled_features\").show(5, truncate=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 10Ô∏è‚É£ Stop Spark\n",
        "# -------------------------------\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "6HAlaWg3NpBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import floor, when, col\n",
        "\n",
        "# Create hour of transaction\n",
        "scaled_data = scaled_data.withColumn(\"Transaction_Hour\", floor(col(\"Time\") / 3600))\n",
        "\n",
        "# Create transaction category\n",
        "scaled_data = scaled_data.withColumn(\n",
        "    \"Amount_Category\",\n",
        "    when(col(\"Amount\") < 10, \"Low\")\n",
        "    .when(col(\"Amount\") < 100, \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ")\n",
        "\n",
        "scaled_data.select(\"Time\", \"Transaction_Hour\", \"Amount\", \"Amount_Category\").show(5)\n",
        "print(\"‚úÖ Feature engineering completed.\")\n",
        "\n",
        "# Drop complex vector columns before saving\n",
        "clean_data = scaled_data.drop(\"features\", \"scaled_features\")\n",
        "\n",
        "# Define output path\n",
        "output_path = \"/content/cleaned_creditcard_data.csv\"\n",
        "\n",
        "# Save as CSV\n",
        "clean_data.write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"‚úÖ Cleaned dataset saved successfully at: {output_path}\")\n",
        "\n",
        "print(\"üìä Data Preprocessing Summary\")\n",
        "print(\"- Missing values handled (numeric: mean, categorical: 'Unknown')\")\n",
        "print(\"- Data types standardized to double\")\n",
        "print(\"- Duplicates removed\")\n",
        "print(\"- Features normalized using StandardScaler\")\n",
        "print(\"- Engineered features: Transaction_Hour, Amount_Category\")\n",
        "print(\"‚úÖ Dataset ready for downstream analytics or ML tasks. üéØ\")\n"
      ],
      "metadata": {
        "id": "_ZuI7YQxNxnw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}