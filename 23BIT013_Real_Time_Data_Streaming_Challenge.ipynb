{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtU+1cEfRaBk20PXik4uLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danushika06/Dpt/blob/main/23BIT013_Real_Time_Data_Streaming_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZG-_lnQI18Q",
        "outputId": "2a1ee614-25fb-4714-9ee2-f22390d83706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n",
            "Train score: 1.0\n",
            "Test score: 0.999\n",
            "Saved model to: /content/sensor_rf_model.joblib\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "print(pyspark.__version__)\n",
        "\n",
        "!pip install -q scikit-learn joblib pandas\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from joblib import dump\n",
        "\n",
        "# Make randomness reproducible (doesn't change structure/printed lines)\n",
        "random.seed(42)\n",
        "\n",
        "def generate_row():\n",
        "    temp = random.uniform(20, 45)\n",
        "    hum = random.uniform(20, 90)\n",
        "    vib = random.uniform(0, 6)\n",
        "    # simple synthetic anomaly rule (for demo)\n",
        "    label = 1 if (vib > 4.0 and temp > 35) else 0\n",
        "    return [temp, hum, vib, label]\n",
        "\n",
        "# Create dataset\n",
        "rows = [generate_row() for _ in range(5000)]\n",
        "df = pd.DataFrame(rows, columns=[\"temperature\",\"humidity\",\"vibration\",\"label\"])\n",
        "\n",
        "X = df[[\"temperature\",\"humidity\",\"vibration\"]]\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=30, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train score:\", clf.score(X_train, y_train))\n",
        "print(\"Test score:\", clf.score(X_test, y_test))\n",
        "\n",
        "# Save model\n",
        "MODEL_PATH = \"/content/sensor_rf_model.joblib\"\n",
        "dump(clf, MODEL_PATH)\n",
        "print(\"Saved model to:\", MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Create directories and a base simulated stream file\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "base = Path(\"/content\")\n",
        "input_dir = base / \"stream_input\"\n",
        "output_dir = base / \"stream_output\"\n",
        "checkpoint_dir = base / \"chkpt_stream\"\n",
        "\n",
        "input_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def generate_sensor_record():\n",
        "    return {\n",
        "        \"sensor_id\": random.choice([\"s1\",\"s2\",\"s3\"]),\n",
        "        \"temperature\": round(random.uniform(20, 45), 2),\n",
        "        \"humidity\": round(random.uniform(20, 90), 2),\n",
        "        \"vibration\": round(random.uniform(0, 6), 2),\n",
        "        \"timestamp\": pd.Timestamp.now()\n",
        "    }\n",
        "\n",
        "# initial microbatch file (will be picked up by Spark as first microbatch)\n",
        "initial_batch = pd.DataFrame([generate_sensor_record() for _ in range(20)])\n",
        "initial_path = input_dir / \"initial_batch.csv\"\n",
        "initial_batch.to_csv(initial_path, index=False)\n",
        "print(\"Wrote initial microbatch:\", str(initial_path))\n",
        "initial_batch.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "t8YyKHUZKJvv",
        "outputId": "2d32b099-0387-4b43-cf8d-83f70ef2d20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote initial microbatch: /content/stream_input/initial_batch.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sensor_id  temperature  humidity  vibration                  timestamp\n",
              "0        s3        43.58     70.71       3.12 2025-10-16 17:56:43.092672\n",
              "1        s1        28.14     66.38       5.45 2025-10-16 17:56:43.092711\n",
              "2        s3        26.10     86.50       0.83 2025-10-16 17:56:43.092719\n",
              "3        s3        38.93     78.52       5.89 2025-10-16 17:56:43.092725\n",
              "4        s1        42.43     87.20       5.32 2025-10-16 17:56:43.092732"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-740b62c3-aad6-481d-b7ce-565323cacaa6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>vibration</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s3</td>\n",
              "      <td>43.58</td>\n",
              "      <td>70.71</td>\n",
              "      <td>3.12</td>\n",
              "      <td>2025-10-16 17:56:43.092672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s1</td>\n",
              "      <td>28.14</td>\n",
              "      <td>66.38</td>\n",
              "      <td>5.45</td>\n",
              "      <td>2025-10-16 17:56:43.092711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s3</td>\n",
              "      <td>26.10</td>\n",
              "      <td>86.50</td>\n",
              "      <td>0.83</td>\n",
              "      <td>2025-10-16 17:56:43.092719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s3</td>\n",
              "      <td>38.93</td>\n",
              "      <td>78.52</td>\n",
              "      <td>5.89</td>\n",
              "      <td>2025-10-16 17:56:43.092725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s1</td>\n",
              "      <td>42.43</td>\n",
              "      <td>87.20</td>\n",
              "      <td>5.32</td>\n",
              "      <td>2025-10-16 17:56:43.092732</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740b62c3-aad6-481d-b7ce-565323cacaa6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-740b62c3-aad6-481d-b7ce-565323cacaa6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-740b62c3-aad6-481d-b7ce-565323cacaa6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-78d1dd59-658c-4177-8e68-8669835f0db4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78d1dd59-658c-4177-8e68-8669835f0db4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-78d1dd59-658c-4177-8e68-8669835f0db4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "initial_batch",
              "summary": "{\n  \"name\": \"initial_batch\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"sensor_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"s3\",\n          \"s1\",\n          \"s2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.390315569075946,\n        \"min\": 20.37,\n        \"max\": 44.96,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          43.58,\n          30.86,\n          26.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.946675028195816,\n        \"min\": 25.25,\n        \"max\": 87.2,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          70.71,\n          47.35,\n          82.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vibration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.686514936660493,\n        \"min\": 0.04,\n        \"max\": 5.89,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          3.12,\n          5.31,\n          5.13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-10-16 17:56:43.092672\",\n        \"max\": \"2025-10-16 17:56:43.093897\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"2025-10-16 17:56:43.092672\",\n          \"2025-10-16 17:56:43.093885\",\n          \"2025-10-16 17:56:43.093873\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Start Spark Structured Streaming to read CSVs in input_dir and apply model predictions\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "from joblib import load\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create Spark session\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ColabStreamingML\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate())\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Input schema\n",
        "schema = StructType([\n",
        "    StructField(\"sensor_id\", StringType(), True),\n",
        "    StructField(\"temperature\", DoubleType(), True),\n",
        "    StructField(\"humidity\", DoubleType(), True),\n",
        "    StructField(\"vibration\", DoubleType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "MODEL_PATH = \"/content/sensor_rf_model.joblib\"\n",
        "\n",
        "# Define foreachBatch processing function\n",
        "def process_batch(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    This function runs for each microbatch.\n",
        "    We load the joblib model inside the function (safe), convert to pandas,\n",
        "    apply model.predict, and print + save the result.\n",
        "    \"\"\"\n",
        "    # Important: don't crash if batch is empty\n",
        "    if batch_df.count() == 0:\n",
        "        print(f\"[batch {batch_id}] empty\")\n",
        "        return\n",
        "\n",
        "    # Convert to pandas\n",
        "    pdf = batch_df.toPandas()\n",
        "    # Ensure features exist and fill NA if any\n",
        "    features = pdf[[\"temperature\",\"humidity\",\"vibration\"]].fillna(0.0)\n",
        "    # Load model inside the batch (safe for pickling/closure)\n",
        "    model = load(MODEL_PATH)\n",
        "    preds = model.predict(features)\n",
        "    pdf[\"prediction\"] = preds\n",
        "    # Add human-readable timestamp for output\n",
        "    pdf[\"processed_at\"] = pd.Timestamp.now()\n",
        "    # Print a small sample\n",
        "    print(f\"\\n=== Microbatch {batch_id} (rows={len(pdf)}) ===\")\n",
        "    print(pdf.head(10).to_string(index=False))\n",
        "    # Append to CSV output directory for record\n",
        "    out_path = os.path.join(\"/content/stream_output\", f\"batch_{batch_id}.csv\")\n",
        "    pdf.to_csv(out_path, index=False)\n",
        "    print(f\"Wrote batch_{batch_id}.csv with {len(pdf)} rows\")\n",
        "\n",
        "# Build streaming DataFrame\n",
        "streaming_df = (spark.readStream\n",
        "                .schema(schema)\n",
        "                .option(\"maxFilesPerTrigger\", 1)   # process one file per microbatch\n",
        "                .csv(str(input_dir)))\n",
        "\n",
        "# Start the query with foreachBatch\n",
        "query = (streaming_df.writeStream\n",
        "         .outputMode(\"append\")\n",
        "         .foreachBatch(process_batch)\n",
        "         .option(\"checkpointLocation\", str(checkpoint_dir))\n",
        "         .start())\n",
        "\n",
        "print(\"‚è≥ Spark streaming started. Waiting for microbatches... (CTRL-C in Colab to interrupt)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v675-V-KMkr",
        "outputId": "611196a1-d343-44c6-93f7-ea16cf050ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Spark streaming started. Waiting for microbatches... (CTRL-C in Colab to interrupt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Simulate new microbatches arriving (write 5 new CSV files, 2-second gaps)\n",
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "def simulate_and_write(batch_idx, nrows=12):\n",
        "    rows = []\n",
        "    for _ in range(nrows):\n",
        "        rows.append({\n",
        "            \"sensor_id\": random.choice([\"s1\",\"s2\",\"s3\"]),\n",
        "            \"temperature\": round(random.uniform(20, 45), 2),\n",
        "            \"humidity\": round(random.uniform(20, 90), 2),\n",
        "            \"vibration\": round(random.uniform(0, 6), 2),\n",
        "            \"timestamp\": pd.Timestamp.now()\n",
        "        })\n",
        "    df_new = pd.DataFrame(rows)\n",
        "    path = (input_dir / f\"batch_{batch_idx}.csv\")\n",
        "    df_new.to_csv(path, index=False)\n",
        "    clear_output(wait=True)\n",
        "    print(f\"üì§ New microbatch written: {path}  (rows={len(df_new)})\")\n",
        "\n",
        "# Simulate 5 microbatches\n",
        "for i in range(5):\n",
        "    simulate_and_write(i+1, nrows=12)\n",
        "    time.sleep(2)   # wait so streaming job processes each file as a microbatch\n",
        "\n",
        "print(\"‚úÖ Simulation finished: 5 microbatches generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X70Bwg2bKRXV",
        "outputId": "9f644d5e-96d7-4047-f50c-30026cbf83e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ New microbatch written: /content/stream_input/batch_5.csv  (rows=12)\n",
            "‚úÖ Simulation finished: 5 microbatches generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Stop the streaming query and Spark session\n",
        "# NOTE: run this after you have processed the batches you want (or if you want to stop)\n",
        "query.stop()\n",
        "spark.stop()\n",
        "print(\"‚úÖ Streaming stopped and Spark session terminated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTZEu0OuKYgF",
        "outputId": "3d1376de-72ce-493a-a559-1f760b105d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"/tmp/ipython-input-1790865637.py\", line 35, in process_batch\n",
            "    if batch_df.count() == 0:\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\", line 1238, in count\n",
            "    return int(self._jdf.count())\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o42.count.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3614)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3613)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
            "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3613)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy31.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Streaming stopped and Spark session terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: List the processed batch files and show a sample\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "files = sorted(glob.glob(\"/content/stream_output/*.csv\"))\n",
        "print(\"Processed batch files:\", files)\n",
        "if files:\n",
        "    sample = pd.read_csv(files[-1])\n",
        "    print(\"\\nSample rows from last batch file:\")\n",
        "    display(sample.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRSJfTKiKaad",
        "outputId": "14d657b5-2afc-46e6-9ab9-728fb5ead64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch files: []\n"
          ]
        }
      ]
    }
  ]
}